{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from colorama import Fore, Back, Style \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = {}\n",
    "raw = \"\"\n",
    "with open(\"onlyTopicsData.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    rawData = data\n",
    "    for bigTopic, topics in data.items():\n",
    "        for topic, text in topics.items():\n",
    "            if text.strip() != \"\":\n",
    "                raw += \" \\n \" + \" \".join(text.strip().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "sent_tokens = []\n",
    "articles = []\n",
    "articleToText = {}\n",
    "with open(\"onlyTopicsData.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for category, topics in data.items():\n",
    "        for topic, text in topics.items():\n",
    "            if text.strip() != \"\" and text.strip() != \" \":\n",
    "                text = \" \".join([w for w in text.split(\" \") if w.strip() != \"\" and \"[\" not in w])\n",
    "                doc = \" \".join([p for p in text.strip().split(\"\\n\") if p.strip() != \"\"])\n",
    "                sentences = [sent for sent in nltk.sent_tokenize(doc) if len(sent) > 5]\n",
    "                article = [topic for _ in range(len(sentences))]\n",
    "                sent_tokens.extend(sentences)\n",
    "                articles.extend(article)\n",
    "                articleToText[topic] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# take as input the tokens and return normalized tokens\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "greeting_in = (\"hello\", \"hi\", \"greetings\", \"sup\", \"yo\", \"hey\", \" what's up\")\n",
    "# array form because random.choice()\n",
    "greeting_out = [\"hi\", \"hey there\", \" hello\", \"I'm glad we are conversing.\"]\n",
    "\n",
    "rejection_words = [\"no\", \"nah\", \"nope\", \"not really\", \"not quite\"]\n",
    "approval_words = [\"yes\", \"yeah\", \"yea\", \"yep\", \"ya\", \"ye\", \"kinda\", \"a little\"]\n",
    "\n",
    "#if user types in greeting, send a greeting out\n",
    "def introduction(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greeting_in:\n",
    "            return random.choice(greeting_out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peteroh/Library/Python/3.7/lib/python/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import sister\n",
    "sentEmbedder = sister.MeanEmbedding(lang=\"en\")\n",
    "def getSentEmbeddings(data):\n",
    "    return np.array([sentEmbedder(sent) for sent in data])\n",
    "\n",
    "#get sentence embeddings, can take a while\n",
    "sentEmbeddings = getSentEmbeddings(sent_tokens)\n",
    "\n",
    "#Entity Recognition\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response\n",
    "def get_user_input():\n",
    "    print(Style.RESET_ALL + \">\", end = \" \")\n",
    "    userinput = input().lower()\n",
    "    print(Fore.RED)\n",
    "    return userinput\n",
    "\n",
    "#original response\n",
    "def response(user_text):\n",
    "    robo_text = ''\n",
    "    values = cosine_similarity(sentEmbedder(user_text).reshape(1,-1), sentEmbeddings)\n",
    "    indexes = values.argsort()[0]\n",
    "    index = values.argsort()[0][-1]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    if (flat[-1] == 0):\n",
    "        robo_text = robo_text + \"I\\'m sorry, I do not understand you. The query you have inputted is incomprehensible. \\n Please try again. \"\n",
    "        return robo_text\n",
    "    else:\n",
    "        original_user_text = user_text\n",
    "        \n",
    "        print(\"I found these articles most similar to your input.\") \n",
    "        \n",
    "        \n",
    "        # print top 5 most related articles\n",
    "        for i in range(1,6): #print 5 most related sentences\n",
    "            index = indexes[-i]\n",
    "            print(\"Article {}: {}\".format(i, articles[index]))\n",
    "            print(\"Similar Sentence:\" + sent_tokens[index])\n",
    "            #entity_recognition(articles[index])\n",
    "            \n",
    "        print()\n",
    "        print(\"Do any of these match your interest?\")\n",
    "        \n",
    "        user_text = get_user_input() # request user's approval\n",
    "        \n",
    "        # validate input\n",
    "        while user_text not in rejection_words and user_text not in approval_words:\n",
    "            print(\"I'm sorry, I did not understand whether you found these interesting. Please say yes or no!\")\n",
    "            user_text = get_user_input()\n",
    "        \n",
    "        if user_text in rejection_words:\n",
    "            print(\"Let me find a few more articles.\")\n",
    "            \n",
    "            # print articles 6-10\n",
    "            for i in range(6,11):\n",
    "                index = indexes[-i]\n",
    "                print(\"Article {}: {}\".format(i, articles[index]))\n",
    "                print(\"Similar Sentence:\" + sent_tokens[index])\n",
    "            \n",
    "            print()\n",
    "            print(\"Do any of these match your interest?\")\n",
    "\n",
    "            user_text = get_user_input() # request user's approval\n",
    "            \n",
    "            while user_text not in rejection_words and user_text not in approval_words:\n",
    "                print(\"I'm sorry, I did not understand whether you found these interesting. Please say yes or no!\")\n",
    "                user_text = get_user_input()\n",
    "            \n",
    "            if user_text in rejection_words:\n",
    "                print(\"I'm sorry I couldn't find any good results. Could you please rephrase your inquiry or try something else?\")\n",
    "                user_text = get_user_input() \n",
    "                response(user_text) # starting over\n",
    "            elif user_text in approval_words:\n",
    "                print(\"Awesome! Please state the article number you would like to explore more.\")\n",
    "                user_text = get_user_input() \n",
    "                next_response(user_text, indexes, 10)\n",
    "                  \n",
    "        elif user_text in approval_words:\n",
    "            print(\"Awesome! Please state the article number you would like to explore more.\")\n",
    "            user_text = get_user_input()\n",
    "            next_response(user_text, original_user_text, indexes, 5)\n",
    "            \n",
    "    return robo_text\n",
    "\n",
    "def next_response(user_text, original_user_text, indexes, rank):\n",
    "    if not user_text.isdigit():\n",
    "        print(\"invalid input\")\n",
    "        return\n",
    "    i = int(re.findall(r\"\\d+\", user_text)[0])\n",
    "    if i > rank:#TODO\n",
    "        print(\"invalid input\")\n",
    "        return\n",
    "    index = indexes[-i]\n",
    "    sentence = sent_tokens[index]\n",
    "    articleText = articleToText[articles[index]]\n",
    "\n",
    "    paragraphs = [p for p in articleText.split(\"\\n\") if p != \"\"]\n",
    "    \n",
    "    articleSents = nltk.sent_tokenize(\" \".join(paragraphs))\n",
    "    articleVectors = getSentEmbeddings(articleSents)\n",
    "    values = cosine_similarity(getSentEmbeddings([sentence,original_user_text]), articleVectors)\n",
    "    indexes = values.argsort()[0]\n",
    "    bestIndex = values.argsort()[0][-1]\n",
    "    output = \"\"\n",
    "    count = 0\n",
    "    while bestIndex < len(values[0]) and values[0][bestIndex] > 0.8 and count < 10:\n",
    "        output += articleSents[bestIndex] + \" \"\n",
    "        bestIndex += 1\n",
    "        count += 1\n",
    "    print(output)\n",
    "              \n",
    "    # entity recognition\n",
    "    print(\"Entity Information: \")\n",
    "    print(\"\")\n",
    "    entity_user = nlp(original_user_text)\n",
    "    entity_article = nlp(articleText.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    #List of people, organization, events, countries entities\n",
    "    people = []\n",
    "    organization = []\n",
    "    events = []\n",
    "    locations = []\n",
    "    \n",
    "    for x in entity_article.ents:\n",
    "        if (x.label_ == \"PERSON\"):\n",
    "            people.append(x.text)\n",
    "        if (x.label_ == \"ORG\"):\n",
    "            organization.append(x.text)\n",
    "        if (x.label_ == \"EVENT\"):\n",
    "            events.append(x.text)\n",
    "        if (x.label_ == \"GPE\"):\n",
    "            locations.append(x.text)\n",
    "\n",
    "    print(\"Top 5 Most Frequent People entities: \")\n",
    "    pprint(Counter(people).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Organization entities: \")\n",
    "    pprint(Counter(organization).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Events entities: \")\n",
    "    pprint(Counter(events).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Location entities: \")\n",
    "    pprint(Counter(locations).most_common(5))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowBot: I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \n",
      " You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \n",
      " KnowBot: If you would like to leave, please type \"bye\".\n",
      "\u001b[0m> systems of linear equations\n",
      "\u001b[31m\n",
      "KnowBot: I found these articles most similar to your input.\n",
      "Article 1: Linear_algebra\n",
      "Similar Sentence:There are many rings for which there are algorithms for solving linear equations and systems of linear equations.\n",
      "Article 2: Linear_algebra\n",
      "Similar Sentence:, z {\\displaystyle x,y,...,z}  is called a system of linear equations or a linear of linear equations form a fundamental part of linear algebra.\n",
      "Article 3: Linear_algebra\n",
      "Similar Sentence:For more details, see Linear equation over a ring.\n",
      "Article 4: Functional_magnetic_resonance_imaging\n",
      "Similar Sentence:This generates a set of linear equations with more equations than unknowns.\n",
      "Article 5: Differential_equation\n",
      "Similar Sentence:Linear differential equations are the differential equations that are linear in the unknown function and its derivatives.\n",
      "\n",
      "Do any of these match your interest?\n",
      "\u001b[0m> yes\n",
      "\u001b[31m\n",
      "Awesome! Please state the article number you would like to explore more.\n",
      "\u001b[0m> 1\n",
      "\u001b[31m\n",
      "There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than the similar algorithms over a field. For more details, see Linear equation over a ring. In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V∗ consisting of linear maps f: V → F where F is the field of scalars. \n",
      "The following similar topics were found in this article: \n",
      "\n",
      "Most common figures: \n",
      "[('linear algebra', 5),\n",
      " ('Cramer', 3),\n",
      " ('René Descartes', 2),\n",
      " ('Leibniz', 1),\n",
      " ('Gabriel Cramer', 1)]\n",
      "Most common subjects: \n",
      "[('Linear', 10),\n",
      " ('−', 4),\n",
      " ('algebras', 2),\n",
      " ('the general linear group', 1),\n",
      " ('hypercomplex', 1)]\n",
      "Most common events: \n",
      "[]\n",
      "Most common locations: \n",
      "[('W', 3), ('Peano', 1), ('F.', 1), ('F', 1), ('∪', 1)]\n",
      "\n",
      "If you would like further information, please type in one of the entity keywords above!\n",
      "Otherwise, please enter a new keyword or question. \n",
      "\n",
      "\u001b[0m> "
     ]
    }
   ],
   "source": [
    "user_exit = False\n",
    "print(\"KnowBot: I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \\n You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \\n KnowBot: If you would like to leave, please type \\\"bye\\\".\")\n",
    "while (user_exit == False):\n",
    "        user_text = get_user_input()\n",
    "        user_text = user_text.lower()\n",
    "        # user want to leave\n",
    "        if (user_text == 'bye'):\n",
    "            user_exit = True\n",
    "            print(\"KnowBot: Bye! Take care and come back soon. \")\n",
    "        # replying to gratitude\n",
    "        elif(user_text == 'thanks' or user_text == 'thank you'):\n",
    "            print(\"KnowBot: You\\'re welcome! Ask me another query!\")\n",
    "        # user needs more instructions\n",
    "        elif (user_text == 'help'):\n",
    "            print(\"KnowBot: I\\'m sorry the instructions were unclear. \\n I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \\n You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \\n If you would like to leave, please type \\\"bye\\\".\")\n",
    "        # user has typed in a greeting\n",
    "        elif (introduction(user_text) != None):\n",
    "            print(\"KnowBot: \" + introduction(user_text))\n",
    "        # user has typed in a keyword, generate a response\n",
    "        else:\n",
    "            print(\"KnowBot: \" , end= \"\")\n",
    "            print(response(user_text))\n",
    "#             sent_tokens.remove(user_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
