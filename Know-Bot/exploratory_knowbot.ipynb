{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from colorama import Fore, Back, Style \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = {}\n",
    "raw = \"\"\n",
    "with open(\"onlyTopicsData.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    rawData = data\n",
    "    for bigTopic, topics in data.items():\n",
    "        for topic, text in topics.items():\n",
    "            if text.strip() != \"\":\n",
    "                raw += \" \\n \" + \" \".join(text.strip().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "sent_tokens = []\n",
    "articles = []\n",
    "articleToText = {}\n",
    "with open(\"onlyTopicsData.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for category, topics in data.items():\n",
    "        for topic, text in topics.items():\n",
    "            if text.strip() != \"\" and text.strip() != \" \":\n",
    "                text = \" \".join([w for w in text.split(\" \") if w.strip() != \"\" and \"[\" not in w])\n",
    "                doc = \" \".join([p for p in text.strip().split(\"\\n\") if p.strip() != \"\"])\n",
    "                sentences = [sent for sent in nltk.sent_tokenize(doc) if len(sent) > 5]\n",
    "                article = [topic for _ in range(len(sentences))]\n",
    "                sent_tokens.extend(sentences)\n",
    "                articles.extend(article)\n",
    "                articleToText[topic] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# take as input the tokens and return normalized tokens\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "greeting_in = (\"hello\", \"hi\", \"greetings\", \"sup\", \"yo\", \"hey\", \" what's up\")\n",
    "# array form because random.choice()\n",
    "greeting_out = [\"hi\", \"hey there\", \" hello\", \"I'm glad we are conversing.\"]\n",
    "\n",
    "rejection_words = [\"no\", \"nah\", \"nope\", \"not really\", \"not quite\"]\n",
    "approval_words = [\"yes\", \"yeah\", \"yea\", \"yep\", \"ya\", \"ye\", \"kinda\", \"a little\"]\n",
    "\n",
    "#if user types in greeting, send a greeting out\n",
    "def introduction(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greeting_in:\n",
    "            return random.choice(greeting_out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peteroh/Library/Python/3.7/lib/python/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "TfidfVect = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "# TfidfVect = TfidfVectorizer(stop_words = 'english', ngram_range=(1,2))\n",
    "tfidf = TfidfVect.fit_transform(sent_tokens)\n",
    "\n",
    "#Entity Recognition\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#response\n",
    "def get_user_input():\n",
    "    print(Style.RESET_ALL + \">\", end = \" \")\n",
    "    userinput = input().lower()\n",
    "    print(Fore.RED)\n",
    "    return userinput\n",
    "\n",
    "#original response\n",
    "def response(user_text):\n",
    "    robo_text = ''\n",
    "    sent_tokens.append(user_text)\n",
    "    values = cosine_similarity(TfidfVect.transform([user_text]), tfidf)\n",
    "    indexes = values.argsort()[0]\n",
    "    index = values.argsort()[0][-1]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    if (flat[-1] == 0):\n",
    "        robo_text = robo_text + \"I\\'m sorry, I do not understand you. The query you have inputted is incomprehensible. \\n Please try again. \"\n",
    "        return robo_text\n",
    "    else:\n",
    "        original_user_text = user_text\n",
    "        \n",
    "        print(\"I found these articles most similar to your input.\") \n",
    "        \n",
    "        \n",
    "        # print top 5 most related articles\n",
    "        for i in range(1,6): #print 5 most related sentences\n",
    "            index = indexes[-i]\n",
    "            print(\"Article {}: {}\".format(i, articles[index]))\n",
    "            print(\"Similar Sentence:\" + sent_tokens[index])\n",
    "            #entity_recognition(articles[index])\n",
    "            \n",
    "        print()\n",
    "        print(\"Do any of these match your interest?\")\n",
    "        \n",
    "        user_text = get_user_input() # request user's approval\n",
    "        \n",
    "        # validate input\n",
    "        while user_text not in rejection_words and user_text not in approval_words:\n",
    "            print(\"I'm sorry, I did not understand whether you found these interesting. Please say yes or no!\")\n",
    "            user_text = get_user_input()\n",
    "        \n",
    "        if user_text in rejection_words:\n",
    "            print(\"Let me find a few more articles.\")\n",
    "            \n",
    "            # print articles 6-10\n",
    "            for i in range(6,11):\n",
    "                index = indexes[-i]\n",
    "                print(\"Article {}: {}\".format(i, articles[index]))\n",
    "                print(\"Similar Sentence:\" + sent_tokens[index])\n",
    "            \n",
    "            print()\n",
    "            print(\"Do any of these match your interest?\")\n",
    "\n",
    "            user_text = get_user_input() # request user's approval\n",
    "            \n",
    "            while user_text not in rejection_words and user_text not in approval_words:\n",
    "                print(\"I'm sorry, I did not understand whether you found these interesting. Please say yes or no!\")\n",
    "                user_text = get_user_input()\n",
    "            \n",
    "            if user_text in rejection_words:\n",
    "                print(\"I'm sorry I couldn't find any good results. Could you please rephrase your inquiry or try something else?\")\n",
    "                user_text = get_user_input() \n",
    "                response(user_text) # starting over\n",
    "            elif user_text in approval_words:\n",
    "                print(\"Awesome! Please state the article number you would like to explore more.\")\n",
    "                user_text = get_user_input() \n",
    "                next_response(user_text, indexes, 10)\n",
    "                  \n",
    "        elif user_text in approval_words:\n",
    "            print(\"Awesome! Please state the article number you would like to explore more.\")\n",
    "            user_text = get_user_input()\n",
    "            next_response(user_text, original_user_text, indexes, 5)\n",
    "            \n",
    "    return robo_text\n",
    "\n",
    "def next_response(user_text, original_user_text, indexes, rank):\n",
    "    if not user_text.isdigit():\n",
    "        print(\"invalid input\")\n",
    "        return\n",
    "    i = int(re.findall(r\"\\d+\", user_text)[0])\n",
    "    if i > rank:#TODO\n",
    "        print(\"invalid input\")\n",
    "        return\n",
    "    index = indexes[-i]\n",
    "    sentence = sent_tokens[index]\n",
    "    articleText = articleToText[articles[index]]\n",
    "\n",
    "    paragraphs = [p for p in articleText.split(\"\\n\") if p != \"\"]\n",
    "    \n",
    "    articleSents = nltk.sent_tokenize(\" \".join(paragraphs))\n",
    "    articleVect = TfidfVectorizer(stop_words = \"english\")\n",
    "    articleTfidf = articleVect.fit_transform(articleSents)\n",
    "    values = cosine_similarity(articleVect.transform([sentence, user_text]), articleTfidf)\n",
    "    indexes = values.argsort()[0]\n",
    "    bestIndex = values.argsort()[0][-1]\n",
    "    output = \"\"\n",
    "    while bestIndex < len(values[0]) and values[0][bestIndex] > 0:\n",
    "        output += articleSents[bestIndex] + \" \"\n",
    "        bestIndex += 1\n",
    "    print(output)\n",
    "              \n",
    "    # entity recognition\n",
    "    print(\"Entity Information: \")\n",
    "    print(\"\")\n",
    "    entity_user = nlp(original_user_text)\n",
    "    entity_article = nlp(articleText)\n",
    "    \n",
    "    # Printing out all Entities found in user text and article text\n",
    "    #pprint([(X.text, X.label_) for X in entity_user.ents])\n",
    "    #print(\"\")\n",
    "    #pprint([(X.text, X.label_) for X in entity_article.ents])\n",
    "    #print(\"\")\n",
    "    \n",
    "    # printing out different number of labels for article text\n",
    "    #labels_article = [X.label_ for X in entity_article.ents]\n",
    "    #pprint(Counter(labels_article))\n",
    "    #print(\"\")\n",
    "    \n",
    "    #List of people, organization, events, countries entities\n",
    "    people = []\n",
    "    organization = []\n",
    "    events = []\n",
    "    locations = []\n",
    "    \n",
    "    for x in entity_article.ents:\n",
    "        if (x.label_ == \"PERSON\"):\n",
    "            people.append(x.text)\n",
    "        if (x.label_ == \"ORG\"):\n",
    "            organization.append(x.text)\n",
    "        if (x.label_ == \"EVENT\"):\n",
    "            events.append(x.text)\n",
    "        if (x.label_ == \"GPE\"):\n",
    "            locations.append(x.text)\n",
    "\n",
    "    print(\"Top 5 Most Frequent People entities: \")\n",
    "    pprint(Counter(people).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Organization entities: \")\n",
    "    pprint(Counter(organization).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Events entities: \")\n",
    "    pprint(Counter(events).most_common(5))\n",
    "    \n",
    "    print(\"Top 5 Most Frequent Location entities: \")\n",
    "    pprint(Counter(locations).most_common(5))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowBot: I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \n",
      " You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \n",
      " KnowBot: If you would like to leave, please type \"bye\".\n",
      "\u001b[0m> who is michelle obama?\n",
      "\u001b[31m\n",
      "KnowBot: I found these articles most similar to your input.\n",
      "Article 1: Blues\n",
      "Similar Sentence:Hosted by President Obama and Mrs. Obama, the show featured performances by B.B.\n",
      "Article 2: Social_networking_service\n",
      "Similar Sentence:JFK was the first president who really understood television, and similarly, Obama is the first president to fully understand the power of social Obama has recognized social media is about creating relationships and connections and therefore used social media to the advantage of presidential election campaigns, in which Obama has dominated his opponents in terms of social media space.\n",
      "Article 3: Political_parties\n",
      "Similar Sentence:Barack Obama briefly had such an advantage between 2009 and 2011.\n",
      "Article 4: Political_party\n",
      "Similar Sentence:Barack Obama briefly had such an advantage between 2009 and 2011.\n",
      "Article 5: Democratic_Party_(United_States)\n",
      "Similar Sentence:Exit polls from the 2008 election showed that voters with a religious affiliation of \"none\" accounted for the 12% of the electorate and overwhelmingly voted for Obama by a 75–25% In his inaugural address, Obama acknowledged atheists by saying that the United States is not just \"Christians and Muslims, Jews and Hindus but non-believers as In the 2012 election cycle, Obama has moderate to high rankings with the Secular Coalition for America whereas the majority of the Republican candidates have ratings in the low-to-failing a diverse group themselves, atheists and secular people may include individuals who are fiscally conservative.\n",
      "\n",
      "Do any of these match your interest?\n",
      "\u001b[0m> no\n",
      "\u001b[31m\n",
      "Let me find a few more articles.\n",
      "Article 6: Richard_Feynman\n",
      "Similar Sentence:His daughter Michelle later made the burial was at Mountain View Cemetery and Mausoleum in Altadena, His last words were: \"I'd hate to die twice.\n",
      "Article 7: Nobel_Prize\n",
      "Similar Sentence:Those hostile to the North and what they considered its deceptive practices during negotiations were deprived of a chance to criticise Lê Đức Thọ, as he declined the Arafat, Shimon Peres, and Yitzhak Rabin received the Peace Prize in 1994 for their efforts in making peace between Israel and Immediately after the award was announced, one of the five Norwegian Nobel Committee members denounced Arafat as a terrorist and Additional misgivings about Arafat were widely expressed in various controversial Peace Prize was that awarded to Barack Obama in Nominations had closed only eleven days after Obama took office as President of the United States, but the actual evaluation occurred over the next eight Obama himself stated that he did not feel deserving of the award, or worthy of the company in which it would place Past Peace Prize laureates were divided, some saying that Obama deserved the award, and others saying he had not secured the achievements to yet merit such an accolade.\n",
      "Article 8: Stone_Age\n",
      "Similar Sentence:A 21st-century series, Chronicles of Ancient Darkness by Michelle Paver tells of two New Stone Age children fighting to fulfil a prophecy and save their clan.\n",
      "Article 9: Democratic_Party_(United_States)\n",
      "Similar Sentence:The most recent was Barack Obama, who was the 44th and held office from 2009 to 2017.\n",
      "Article 10: Politicization_of_science\n",
      "Similar Sentence:In 2011, during his State of the Union speech, Obama discussed his dissatisfaction of the relationships between organized science, private economic interests, and the government.\n",
      "\n",
      "Do any of these match your interest?\n",
      "\u001b[0m> article 4\n",
      "\u001b[31m\n",
      "I'm sorry, I did not understand whether you found these interesting. Please say yes or no!\n",
      "\u001b[0m> yes\n",
      "\u001b[31m\n",
      "Awesome! Please state the article number you would like to explore more.\n",
      "\u001b[0m> 4\n",
      "\u001b[31m\n",
      "The field of effective descriptive set theory is between set theory and recursion theory.\n",
      "It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory.\n",
      "In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (\"relativizing\") it to make it more broadly applicable.\n",
      "\n",
      "\u001b[0m> quit\n",
      "\u001b[31m\n",
      "KnowBot: I found these articles most similar to your input.\n",
      "Article 1: Automobile\n",
      "Similar Sentence:Maybach quit DMG shortly thereafter and opened a business of his own.\n",
      "Article 2: Hypnotism\n",
      "Similar Sentence:The technique is often used to increase motivation for a diet, to quit smoking, or to reduce stress.\n",
      "Article 3: Pop_country\n",
      "Similar Sentence:(Running from 1980 to 1982 – Mandrell had to quit the show because of health reasons.)\n",
      "Article 4: Polyrhythm\n",
      "Similar Sentence:The Aaliyah song \"Quit Hatin\" uses 98 against 44 in the chorus.\n",
      "Article 5: Vannevar_Bush\n",
      "Similar Sentence:Bush preferred to quit rather than study a subject that did not interest subsequently enrolled in the Massachusetts Institute of Technology (MIT) electrical engineering program.\n",
      "\n",
      "Do any of these match your interest?\n",
      "\u001b[0m> "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c9d537be661e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KnowBot: \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#             sent_tokens.remove(user_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a8590d7f57fe>\u001b[0m in \u001b[0;36mresponse\u001b[0;34m(user_text)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do any of these match your interest?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0muser_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# request user's approval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# validate input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a8590d7f57fe>\u001b[0m in \u001b[0;36mget_user_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\">\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muserinput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muserinput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_exit = False\n",
    "print(\"KnowBot: I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \\n You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \\n KnowBot: If you would like to leave, please type \\\"bye\\\".\")\n",
    "while (user_exit == False):\n",
    "        user_text = get_user_input()\n",
    "        user_text = user_text.lower()\n",
    "        # user want to leave\n",
    "        if (user_text == 'bye'):\n",
    "            user_exit = True\n",
    "            print(\"KnowBot: Bye! Take care and come back soon. \")\n",
    "        # replying to gratitude\n",
    "        elif(user_text == 'thanks' or user_text == 'thank you'):\n",
    "            print(\"KnowBot: You\\'re welcome! Ask me another query!\")\n",
    "        # user needs more instructions\n",
    "        elif (user_text == 'help'):\n",
    "            print(\"KnowBot: I\\'m sorry the instructions were unclear. \\n I am a robot designed to answer queries you have about the following subjects: Mathematics, Science, Music, Politics, History (USA), Computer Science. \\n You can type in keyword(s) (i.e. multiplication, linear algebra, boolean, 1844) to learn more about that subject. \\n If you would like to leave, please type \\\"bye\\\".\")\n",
    "        # user has typed in a greeting\n",
    "        elif (introduction(user_text) != None):\n",
    "            print(\"KnowBot: \" + introduction(user_text))\n",
    "        # user has typed in a keyword, generate a response\n",
    "        else:\n",
    "            print(\"KnowBot: \" , end= \"\")\n",
    "            print(response(user_text))\n",
    "#             sent_tokens.remove(user_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
