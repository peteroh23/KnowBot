{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "#for visualization of Entity detection importing displacy from spacy:\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "# word vector representation\n",
    "import en_core_web_sm\n",
    "\n",
    "# for text classification\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sentence tokenization\n",
    "\n",
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = English()\n",
    "\n",
    "# # Create the pipeline 'sentencizer' component\n",
    "# sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# # Add the component to the pipeline\n",
    "# nlp.add_pipe(sbd)\n",
    "\n",
    "# text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "# Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "# #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "# doc = nlp(text)\n",
    "\n",
    "# print(type(doc))\n",
    "\n",
    "# # create list of sentence tokens\n",
    "# sents_list = []\n",
    "# for sent in doc.sents:\n",
    "#     sents_list.append(sent.text)\n",
    "# print(sents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if not word.is_stop:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = ' '.join([str(tok) for tok in filtered_sent])\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner ran\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
    "\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
    "\n",
    "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nytimes, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docp = nlp (\" In pursuit of a wall, President Trump ran into one.\")\n",
    "\n",
    "for chunk in docp.noun_chunks:\n",
    "   print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docp, style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting AI prac code with Spacy\n",
    "import json\n",
    "\n",
    "# building text from wikipedia json \n",
    "raw = \"\"\n",
    "with open(\"onlyTopicsData.json\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for bigTopic, topics in data.items():\n",
    "        for topic, text in topics.items():\n",
    "            raw += \" \\n \" + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_word_lst = STOP_WORDS\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punct_lst = string.punctuation\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def tokenizer(text):\n",
    "    parser = English()\n",
    "    # Create the pipeline 'sentencizer' component\n",
    "    sbd = parser.create_pipe('sentencizer')\n",
    "\n",
    "    # Add the component to the pipeline\n",
    "    parser.add_pipe(sbd)\n",
    "    \n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    doc = parser(text)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    #doc = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    #doc = [ word for word in mytokens if word not in stop_word_lst and word not in punct_lst ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47215311\n",
      "[' \\n \\n\\n\\n\\n Mathematics from Greek μάθημα máthēma knowledge study learning includes the study of such topics as quantity number theory),[1 structure algebra),[2 space geometry),[1 and change mathematical analysis).[3][4][5 It has no generally accepted definition.[6][7 \\n\\n Mathematicians seek and use patterns[8][9 to formulate new conjectures they resolve the truth or falsity of conjectures by mathematical proof', 'When mathematical structures are good models of real phenomena mathematical reasoning can be used to provide insight or predictions about nature']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "num_chars = len(raw)\n",
    "print(num_chars)\n",
    "\n",
    "num_chars_tokenized = 0\n",
    "start = 0\n",
    "end = 100000\n",
    "\n",
    "sent_lst = []\n",
    "\n",
    "while num_chars_tokenized <= 500000:\n",
    "    text = raw[start:end]\n",
    "    doc = tokenizer(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        new_sent = \" \".join([str(tok) for tok in sent if tok not in stop_word_lst and str(tok) not in punct_lst])\n",
    "        sent_lst.append(new_sent)\n",
    "        \n",
    "        \n",
    "    start = end\n",
    "    end += 100000\n",
    "    num_chars_tokenized += 100000\n",
    "    \n",
    "print(sent_lst[0:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "TfidfVect = TfidfVectorizer(tokenizer = tokenizer)\n",
    "tfidf = TfidfVect.fit_transform(sent_lst)\n",
    "\n",
    "print(type(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> what is mathematics\n",
      "I'm sorry, I do not understand you. The query you have inputted is incomprehensible. \n",
      " Please try again. \n",
      "> math\n",
      "I'm sorry, I do not understand you. The query you have inputted is incomprehensible. \n",
      " Please try again. \n",
      "> "
     ]
    }
   ],
   "source": [
    "def response (user_text):\n",
    "    robo_text = ''\n",
    "    sent_lst.append(user_text)\n",
    "    TfidfVect = TfidfVectorizer(tokenizer = tokenizer, stop_words = 'english')\n",
    "    tfidf = TfidfVect.fit_transform(sent_lst)\n",
    "    values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    index = values.argsort()[0][-2]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    new_tfidf = flat[-2]\n",
    "    if (new_tfidf == 0):\n",
    "        robo_text = robo_text + \"I\\'m sorry, I do not understand you. The query you have inputted is incomprehensible. \\n Please try again. \"\n",
    "        return robo_text\n",
    "    else:\n",
    "        robo_text = robo_text + sent_tokens[index]\n",
    "        return robo_text\n",
    "    \n",
    "\n",
    "user_exit = False\n",
    "\n",
    "while not user_exit:\n",
    "    print(\">\", end=\" \")\n",
    "    user_text = input()\n",
    "    user_text = user_text.lower()\n",
    "    print(response(user_text))\n",
    "    sent_lst.remove(user_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
